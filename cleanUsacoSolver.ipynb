{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "4839ff9f-351f-4029-b92b-72a0a7f116fa",
   "metadata": {},
   "source": [
    "**Imports and Installations**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 190,
   "id": "e4f20396-18f5-460d-b174-70c986514ea7",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%capture --no-stderr\n",
    "%pip install -U langgraph langsmith langchain_anthropic datasets langchain langchainhub\n",
    "%pip install --upgrade --quiet  rank_bm25\n",
    "%pip install -qU langchain-openai"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 191,
   "id": "29bc125e-820e-462a-a8e8-688d2c1a81ff",
   "metadata": {},
   "outputs": [],
   "source": [
    "import getpass\n",
    "import os\n",
    "import zipfile\n",
    "import datasets\n",
    "import requests\n",
    "import re\n",
    "import multiprocessing\n",
    "import queue\n",
    "import subprocess\n",
    "import sys\n",
    "import time\n",
    "import traceback\n",
    "\n",
    "from typing import Annotated\n",
    "from typing_extensions import TypedDict\n",
    "from langgraph.graph.message import AnyMessage, add_messages\n",
    "from langchain_core.messages import AIMessage, HumanMessage, ToolMessage\n",
    "from langchain_core.language_models import BaseChatModel\n",
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "from pydantic import BaseModel, Field\n",
    "from langchain import hub\n",
    "from langchain_anthropic import ChatAnthropic\n",
    "from langchain_community.retrievers import BM25Retriever\n",
    "from langchain_core.runnables import RunnableConfig\n",
    "from langgraph.checkpoint.memory import MemorySaver\n",
    "from langgraph.graph import END, StateGraph, START\n",
    "from langchain_core.tracers.context import tracing_v2_enabled\n",
    "from langsmith import Client\n",
    "from langchain_openai import ChatOpenAI"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "52f6b6d9-9b87-441e-8a43-c2147478467a",
   "metadata": {},
   "source": [
    "**Set up API Keys and LangSmith tracing**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 192,
   "id": "804b0f86-b0db-4173-9603-f57d14958459",
   "metadata": {},
   "outputs": [],
   "source": [
    "def _get_env(var: str):\n",
    "    if not os.environ.get(var):\n",
    "        os.environ[var] = getpass.getpass(f\"{var}: \")\n",
    "\n",
    "\n",
    "_get_env(\"OPENAI_API_KEY\")\n",
    "_get_env(\"LANGCHAIN_API_KEY\")\n",
    "_get_env(\"ANTHROPIC_API_KEY\")\n",
    "\n",
    "os.environ[\"LANGCHAIN_TRACING_V2\"] = \"true\"\n",
    "os.environ[\"LANGCHAIN_PROJECT\"] = \"usacoV2\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fb13e6bc-63cd-4b7a-a77b-5809505fdf7b",
   "metadata": {},
   "source": [
    "**Get the USACO dataset**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 193,
   "id": "2385dcee-2601-4109-bdcc-9c2fabb09fe5",
   "metadata": {},
   "outputs": [],
   "source": [
    "usaco_url = \"https://storage.googleapis.com/benchmarks-artifacts/usaco/usaco_sampled_with_tests.zip\"\n",
    "zip_path = \"usaco.zip\"\n",
    "extract_path = \"usaco_datasets\"\n",
    "\n",
    "response = requests.get(usaco_url)\n",
    "with open(zip_path, \"wb\") as file:\n",
    "    file.write(response.content)\n",
    "\n",
    "with zipfile.ZipFile(zip_path, \"r\") as zip_ref:\n",
    "    zip_ref.extractall(extract_path)\n",
    "\n",
    "os.remove(zip_path)\n",
    "\n",
    "#ds = datasets.load_from_disk(os.path.join(extract_path, \"usaco_v3_sampled_with_tests\"))\n",
    "ds = datasets.load_from_disk(\"/Users/stevenyu/codeSheepcode/usaco_data/datasets/usaco_v3\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c4323c1c-3e40-4b7b-b4c2-c01dba1c2b6d",
   "metadata": {},
   "source": [
    "**filter_python_code**: Gets a python-only output from an LLM raw output (mainly used for GPT-4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "id": "2d7a906c-4b4b-4bf0-a2d0-fd038521c51b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def filter_python_code(output: str) -> str:\n",
    "    \"\"\"\n",
    "    Filters the model's output to extract only the Python code contained within ```python and ``` delimiters.\n",
    "    \n",
    "    Parameters:\n",
    "        output (str): The raw output from the model.\n",
    "\n",
    "    Returns:\n",
    "        str: Filtered output containing only the Python code.\n",
    "    \"\"\"\n",
    "    # Locate code block between ```python and ```\n",
    "    code_block = re.search(r\"```python(.*?)```\", output, re.DOTALL)\n",
    "    \n",
    "    # If code block is found, return the content; otherwise, return an empty string\n",
    "    if code_block:\n",
    "        return code_block.group(1).strip()\n",
    "    else:\n",
    "        return output"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7ede5b0d-9eb6-486b-ba23-028c53cc123c",
   "metadata": {},
   "source": [
    "**exec_program**: Function for running local code\n",
    "\n",
    "**check_correctness**: Function for checking correctness of code output\n",
    "\n",
    "**Sets timeout to 2 seconds**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "id": "b5a6838b-c558-4973-b2d2-d54f73b99664",
   "metadata": {},
   "outputs": [],
   "source": [
    "multiprocessing.set_start_method(\"fork\", force=True)\n",
    "# WARNING\n",
    "# This program exists to execute untrusted model-generated code. Although\n",
    "# it is highly unlikely that model-generated code will do something overtly\n",
    "# malicious in response to this test suite, model-generated code may act\n",
    "# destructively due to a lack of model capability or alignment.\n",
    "# Users are strongly encouraged to sandbox this evaluation suite so that it\n",
    "# does not perform destructive actions on their host or network.\n",
    "# Proceed at your own risk:\n",
    "\n",
    "\n",
    "def exec_program(q, program, input_data, expected_output, timeout):\n",
    "    try:\n",
    "        start_time = time.time()\n",
    "        process = subprocess.Popen(\n",
    "            [sys.executable, \"-c\", program],\n",
    "            stdin=subprocess.PIPE,\n",
    "            stdout=subprocess.PIPE,\n",
    "            stderr=subprocess.PIPE,\n",
    "            text=True,\n",
    "        )\n",
    "        stdout, stderr = process.communicate(input=input_data, timeout=timeout)\n",
    "        if time.time() - start_time > timeout:\n",
    "            raise TimeoutError(\"Execution timed out.\")\n",
    "        if process.returncode != 0:\n",
    "            q.put(f\"failed: {stderr}\")\n",
    "        else:\n",
    "            if stdout.strip() == expected_output.strip():\n",
    "                q.put(\"passed\")\n",
    "            else:\n",
    "                q.put(f\"wrong answer. Expected '{expected_output}', got '{stdout}'\")\n",
    "    except subprocess.TimeoutExpired:\n",
    "        process.kill()\n",
    "        q.put(\"timed out\")\n",
    "    except Exception:\n",
    "        q.put(f\"failed: {traceback.format_exc()}\")\n",
    "\n",
    "\n",
    "def check_correctness(\n",
    "    program: str, input_data: str, expected_output: str, timeout: float\n",
    ") -> str:\n",
    "    filtered_program = filter_python_code(program)\n",
    "    q = multiprocessing.Queue()\n",
    "    process = multiprocessing.Process(\n",
    "        target=exec_program, args=(q, program, input_data, expected_output, timeout)\n",
    "    )\n",
    "    process.start()\n",
    "    process.join(timeout=timeout + 1)\n",
    "    if process.is_alive():\n",
    "        process.terminate()\n",
    "        process.join()\n",
    "        result = \"timed out\"\n",
    "    else:\n",
    "        try:\n",
    "            result = q.get_nowait()\n",
    "        except queue.Empty:\n",
    "            result = \"no result returned\"\n",
    "    return result\n",
    "\n",
    "timeout = 2"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0044f2ac-9617-4235-9411-24586412817b",
   "metadata": {},
   "source": [
    "**State for each example in the dataset**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "id": "8c50034d-fdde-4f68-9a2f-f2113bed661b",
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyError",
     "evalue": "'test_cases'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[123], line 4\u001b[0m\n\u001b[1;32m      1\u001b[0m input_states \u001b[38;5;241m=\u001b[39m [\n\u001b[1;32m      2\u001b[0m     {\n\u001b[1;32m      3\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmessages\u001b[39m\u001b[38;5;124m\"\u001b[39m: [(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124muser\u001b[39m\u001b[38;5;124m\"\u001b[39m, row[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mdescription\u001b[39m\u001b[38;5;124m\"\u001b[39m])],\n\u001b[0;32m----> 4\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtest_cases\u001b[39m\u001b[38;5;124m\"\u001b[39m: \u001b[43mrow\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mtest_cases\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m]\u001b[49m,\n\u001b[1;32m      5\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mruntime_limit\u001b[39m\u001b[38;5;124m\"\u001b[39m: row[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mruntime_limit\u001b[39m\u001b[38;5;124m\"\u001b[39m],\n\u001b[1;32m      6\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mstatus\u001b[39m\u001b[38;5;124m\"\u001b[39m: \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124min_progress\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[1;32m      7\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mproblem_level\u001b[39m\u001b[38;5;124m\"\u001b[39m: row[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mproblem_level\u001b[39m\u001b[38;5;124m\"\u001b[39m],\n\u001b[1;32m      8\u001b[0m     }\n\u001b[1;32m      9\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m row \u001b[38;5;129;01min\u001b[39;00m ds\n\u001b[1;32m     10\u001b[0m ]\n",
      "\u001b[0;31mKeyError\u001b[0m: 'test_cases'"
     ]
    }
   ],
   "source": [
    "input_states = [\n",
    "    {\n",
    "        \"messages\": [(\"user\", row[\"description\"])],\n",
    "        \"test_cases\": row[\"test_cases\"],\n",
    "        \"runtime_limit\": row[\"runtime_limit\"],\n",
    "        \"status\": \"in_progress\",\n",
    "        \"problem_level\": row[\"problem_level\"],\n",
    "    }\n",
    "    for row in ds\n",
    "]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "49f7d71d-f5a1-4486-9e06-b7abb3986c08",
   "metadata": {},
   "source": [
    "**TestCase**: Class for test cases\n",
    "\n",
    "**State**: Class for states of the graph"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "id": "464583c4-9985-420f-8044-42e9b0e9d682",
   "metadata": {},
   "outputs": [],
   "source": [
    "class TestCase(TypedDict):\n",
    "    inputs: str\n",
    "    outputs: str\n",
    "\n",
    "\n",
    "class State(TypedDict):\n",
    "    # Candidate for retrieval + formatted fetched examples as \"memory\"\n",
    "    candidate: AIMessage\n",
    "    examples: str\n",
    "    messages: Annotated[list[AnyMessage], add_messages]\n",
    "    test_cases: list[TestCase]\n",
    "    runtime_limit: int\n",
    "    status: str"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ccc7ceb1-ee19-496f-9ebe-af19d13a7073",
   "metadata": {},
   "source": [
    "**format_tool_message**: formats raw output of Claude\n",
    "\n",
    "**evaluate**: Checks if the LM's code passes the test cases"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "id": "f4078c8b-c88c-4292-8bca-96ad19ab0c68",
   "metadata": {},
   "outputs": [],
   "source": [
    "def format_tool_message(response: str, ai_message: AIMessage):\n",
    "    return ToolMessage(\n",
    "        content=response + \"\\nMake all fixes using the writePython tool.\",\n",
    "        tool_call_id=ai_message.tool_calls[0][\"id\"],\n",
    "    )\n",
    "\n",
    "def evaluate(state: State):\n",
    "    test_cases = state[\"test_cases\"]\n",
    "    ai_message: AIMessage = state[\"messages\"][-1]\n",
    "    if not ai_message.tool_calls:\n",
    "        return {\n",
    "            \"messages\": [\n",
    "                HumanMessage(\n",
    "                    content=\"No code submitted. Please try again using the correct python code.\"\n",
    "                )\n",
    "            ]\n",
    "        }\n",
    "    try:\n",
    "        code = ai_message.tool_calls[0][\"args\"][\"code\"]\n",
    "    except Exception as e:\n",
    "        return {\"messages\": [format_tool_message(repr(e), ai_message)]}\n",
    "    num_test_cases = len(test_cases)\n",
    "    succeeded = 0\n",
    "    test_results = []\n",
    "    # TODO: Multiprocess\n",
    "    for test_case in test_cases:\n",
    "        input_data = test_case[\"inputs\"]\n",
    "        expected_output = test_case[\"outputs\"]\n",
    "        test_result = check_correctness(code, input_data, expected_output, timeout)\n",
    "        test_results.append(test_result)\n",
    "        if test_result == \"passed\":\n",
    "            succeeded += 1\n",
    "    pass_rate = succeeded / num_test_cases if num_test_cases else \"N/A\"\n",
    "    if pass_rate == 1:\n",
    "        return {\"status\": \"success\"}\n",
    "\n",
    "    responses = \"\\n\".join(\n",
    "        [f\"<test id={i}>\\n{r}\\n</test>\" for i, r in enumerate(test_results)]\n",
    "    )\n",
    "    response = f\"Incorrect submission. Please respond with updated code.\\nPass rate: {succeeded}/{num_test_cases}\\nResults:\\n{responses}\"\n",
    "    formatted_message = format_tool_message(response, ai_message)\n",
    "    return {\"messages\": [formatted_message]}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7db1f8bc-d117-4d74-a57f-e56f4a78fbd3",
   "metadata": {},
   "source": [
    "**writePython**: Gives the LM a specific format to generate the solution code\n",
    "\n",
    "**Solver**: Special class that uses its LM to answer a given prompt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 170,
   "id": "dd5a91f0-84cb-4fad-9bb1-b5d1254d692e",
   "metadata": {},
   "outputs": [],
   "source": [
    "class writePython(BaseModel):\n",
    "    \"\"\"Write python code that resolves the problem.\"\"\"\n",
    "\n",
    "    reasoning: str = Field(..., description=\"Conceptual solution.\")\n",
    "    pseudocode: str = Field(..., description=\"Detailed English pseudocode.\")\n",
    "    code: str = Field(..., description=\"Valid Python 3 solution to the problem\")\n",
    "\n",
    "class modifyPython(BaseModel):\n",
    "    \"\"\"Modifies existing code\"\"\"\n",
    "\n",
    "    analysis: str = Field(..., description=\"Analysis of previous attempt.\")\n",
    "    code: str = Field(..., description=\"Improved Python 3 solution to the problem\")\n",
    "\n",
    "\n",
    "class Solver:\n",
    "    def __init__(self, llm: BaseChatModel, prompt: ChatPromptTemplate):\n",
    "        self.runnable = prompt | llm.bind_tools([writePython], tool_choice=\"writePython\")\n",
    "\n",
    "    def __call__(self, state: State) -> dict:\n",
    "        # Our agent only can see the \"messages\" and will ignore the test info\n",
    "        inputs = {\"messages\": state[\"messages\"]}\n",
    "        has_examples = bool(state.get(\"examples\"))\n",
    "        output_key = \"candidate\"  # Used in the draft node\n",
    "        if has_examples:\n",
    "            output_key = \"messages\"\n",
    "            # Used in the solve node\n",
    "            inputs[\"examples\"] = state[\"examples\"]\n",
    "        response = self.runnable.invoke(inputs)\n",
    "        return {output_key: response}\n",
    "\n",
    "class Modifier:\n",
    "    def __init__(self, llm: BaseChatModel):\n",
    "        self.runnable = llm.bind_tools([modifyPython], tool_choice=\"modifyPython\")\n",
    "\n",
    "    def __call__(self, state: State) -> dict:\n",
    "        # Extract the last AI message which should contain the code to evaluate\n",
    "        ai_message: AIMessage = state[\"messages\"][-1]\n",
    "        test_cases = state[\"test_cases\"]\n",
    "\n",
    "        # Extract code from the message\n",
    "        code = None\n",
    "        if not ai_message.tool_calls:\n",
    "            filtered_code = filter_python_code(ai_message.content)\n",
    "            if not filtered_code:\n",
    "                return {\n",
    "                    \"messages\": [\n",
    "                        HumanMessage(\n",
    "                            content=\"No evaluatable code found in the submission. Please provide valid Python code.\"\n",
    "                        )\n",
    "                    ],\n",
    "                    \"status\": \"invalid_submission\"\n",
    "                }\n",
    "            code = filtered_code\n",
    "        else:\n",
    "            try:\n",
    "                if \"code\" in ai_message.tool_calls[0][\"args\"]:\n",
    "                    code = ai_message.tool_calls[0][\"args\"][\"code\"]\n",
    "                else:\n",
    "                    raise ValueError(\"No code found in tool arguments.\")\n",
    "            except Exception as e:\n",
    "                return {\n",
    "                    \"messages\": [format_tool_message(repr(e), ai_message)],\n",
    "                    \"status\": \"error\"\n",
    "                }\n",
    "            \n",
    "        num_test_cases = len(test_cases)\n",
    "        succeeded = 0\n",
    "        test_results = []\n",
    "        # TODO: Multiprocess\n",
    "        for test_case in test_cases:\n",
    "            input_data = test_case[\"inputs\"]\n",
    "            expected_output = test_case[\"outputs\"]\n",
    "            test_result = check_correctness(code, input_data, expected_output, timeout)\n",
    "            test_results.append(test_result)\n",
    "            if test_result == \"passed\":\n",
    "                succeeded += 1\n",
    "        pass_rate = succeeded / num_test_cases if num_test_cases else \"N/A\"\n",
    "        if pass_rate == 1:\n",
    "            return {\"status\": \"success\"}\n",
    "\n",
    "        responses = \"\\n\".join(\n",
    "            [f\"<test id={i}>\\n{r}\\n</test>\" for i, r in enumerate(test_results)]\n",
    "        )\n",
    "        \n",
    "        problem = state[\"messages\"][0]\n",
    "        \n",
    "        response = self.runnable.invoke(\"\"\"Write an analysis of the previous attempt based on the problem, code, and test case results, and create a modified version of the code.\\n\n",
    "        Problem: {problem}\\n\\n\n",
    "        Code: {code}\\n\\n\n",
    "        Test Case Results: {responses}\n",
    "        \"\"\".format(problem=problem, code=code, responses=responses))\n",
    "\n",
    "        return {\"messages\": response}\n",
    "        \n",
    "\n",
    "class Evaluator:\n",
    "    def __init__(self, llm: BaseChatModel):\n",
    "        self.llm = llm\n",
    "\n",
    "    def __call__(self, state: State) -> dict:\n",
    "        # Our agent only can see the \"messages\" and will ignore the test info\n",
    "        \"\"\"\n",
    "        Evaluates code submitted by another LLM against test cases.\n",
    "        \n",
    "        Args:\n",
    "            state (State): Contains messages, test cases, and optional examples\n",
    "            \n",
    "        \"\"\"\n",
    "        # Extract the last AI message which should contain the code to evaluate\n",
    "        ai_message: AIMessage = state[\"messages\"][-1]\n",
    "        test_cases = state[\"test_cases\"]\n",
    "\n",
    "        # Extract code from the message\n",
    "        code = None\n",
    "        if not ai_message.tool_calls:\n",
    "            filtered_code = filter_python_code(ai_message.content)\n",
    "            if not filtered_code:\n",
    "                return {\n",
    "                    \"messages\": [\n",
    "                        HumanMessage(\n",
    "                            content=\"No evaluatable code found in the submission. Please provide valid Python code.\"\n",
    "                        )\n",
    "                    ],\n",
    "                    \"status\": \"invalid_submission\"\n",
    "                }\n",
    "            code = filtered_code\n",
    "        else:\n",
    "            try:\n",
    "                if \"code\" in ai_message.tool_calls[0][\"args\"]:\n",
    "                    code = ai_message.tool_calls[0][\"args\"][\"code\"]\n",
    "                else:\n",
    "                    raise ValueError(\"No code found in tool arguments.\")\n",
    "            except Exception as e:\n",
    "                return {\n",
    "                    \"messages\": [format_tool_message(repr(e), ai_message)],\n",
    "                    \"status\": \"error\"\n",
    "                }\n",
    "            \n",
    "        num_test_cases = len(test_cases)\n",
    "        succeeded = 0\n",
    "        test_results = []\n",
    "        # TODO: Multiprocess\n",
    "        for test_case in test_cases:\n",
    "            input_data = test_case[\"inputs\"]\n",
    "            expected_output = test_case[\"outputs\"]\n",
    "            test_result = check_correctness(code, input_data, expected_output, timeout)\n",
    "            test_results.append(test_result)\n",
    "            if test_result == \"passed\":\n",
    "                succeeded += 1\n",
    "        pass_rate = succeeded / num_test_cases if num_test_cases else \"N/A\"\n",
    "        if pass_rate == 1:\n",
    "            return {\"status\": \"success\"}\n",
    "\n",
    "        responses = \"\\n\".join(\n",
    "            [f\"<test id={i}>\\n{r}\\n</test>\" for i, r in enumerate(test_results)]\n",
    "        )\n",
    "        \n",
    "        problem = state[\"messages\"][0]\n",
    "        \n",
    "        analysis = self.llm.invoke(\"\"\"Write an analysis of the previous attempt based on the problem, code, and test case results, and create a modified version of the code.\\n\n",
    "        Problem: {problem}\\n\\n\n",
    "        Code: {code}\\n\\n\n",
    "        Test Case Results: {responses}\n",
    "        \"\"\".format(problem=problem, code=code, responses=responses))\n",
    "\n",
    "        response = f\"The previous submission failed at least one test case either by error or timeout. Please respond with updated code.\\nPass rate: {succeeded}/{num_test_cases}\\nResults:\\n{responses}\\nAnalysis:\\n{analysis}\"\n",
    "        formatted_message = format_tool_message(response, ai_message)\n",
    "\n",
    "        return {\"messages\": [formatted_message]}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "97081509-fd26-41ab-b762-a93df86dd2ac",
   "metadata": {},
   "source": [
    "**1. Sets the following prompt:**\n",
    "\n",
    "================================ System Message ================================\n",
    "\n",
    "You are a world-class competitive programmer.\n",
    "Please reply with a Python 3 solution to the problem below. \n",
    "First, reason through the problem and conceptualize a solution.\n",
    "Then write detailed pseudocode to uncover any potential logical errors or omissions.\n",
    "Finally output the working Python code for your solution, ensuring to fix any errors uncovered while writing pseudocode.\n",
    "\n",
    "No outside libraries are allowed.{examples}\n",
    "\n",
    "============================= Messages Placeholder =============================\n",
    "\n",
    "{messages}\n",
    "\n",
    "**2. Sets the LM to Claude 3 Opus**\n",
    "\n",
    "**3. Creates a solver and draft solver**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 171,
   "id": "e055b6f2-f2bf-44d6-83d4-f1199e51201c",
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt = hub.pull(\"wfh/usaco-draft-solver\")\n",
    "llm_claude = ChatAnthropic(model=\"claude-3-opus-20240229\", max_tokens = 4096)\n",
    "llm_openai = ChatOpenAI(model=\"gpt-4o\") \n",
    "\n",
    "draft_solver = Solver(llm_claude, prompt.partial(examples=\"\"))\n",
    "solver = Solver(llm_openai, prompt)\n",
    "modifier = Modifier(llm_claude)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "997d7a82-3802-42d8-9dc7-5f1cb3e7e2c2",
   "metadata": {},
   "source": [
    "**format_example**: Formats each problem in the dataset so that the retriever can handle it\n",
    "\n",
    "**retriever**: Uses BM25 to find the best examples of code for the LM to use"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 161,
   "id": "abb11179-d8b9-4fab-b68c-549a037d67b3",
   "metadata": {},
   "outputs": [],
   "source": [
    "def format_example(row):\n",
    "    question = row[\"description\"]\n",
    "    answer = row[\"solution\"]\n",
    "    return f\"\"\"<problem>\n",
    "{question}\n",
    "</problem>\n",
    "<solution>\n",
    "{answer}\n",
    "</solution>\"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d1bf3fb4-a2d4-4a11-8a7e-1c456c9626d1",
   "metadata": {},
   "source": [
    "**retrieve_examples**: Function that finds the most relevant previous problems from episodic memory based on the current State of the graph, including a draft code and the input problem"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 162,
   "id": "c5bfff4c-94a6-4a1b-a9d9-29a8b17c28e0",
   "metadata": {},
   "outputs": [],
   "source": [
    "def retrieve_examples(state: State, config: RunnableConfig):\n",
    "    top_k = config[\"configurable\"].get(\"k\") or 2\n",
    "    ai_message: AIMessage = state[\"candidate\"]\n",
    "    id = config[\"configurable\"].get(\"thread_id\")\n",
    "\n",
    "    test_ds = [row for row in ds if row[\"cp_id\"] != id]\n",
    "    retriever = BM25Retriever.from_texts([format_example(row) for row in test_ds])\n",
    "    \n",
    "    if not ai_message.tool_calls:\n",
    "        # We err here. To make more robust, you could loop back\n",
    "        raise ValueError(\"Draft agent did not produce a valid code block\")\n",
    "    code = ai_message.tool_calls[0][\"args\"][\"code\"]\n",
    "    examples_str = \"\\n\".join(\n",
    "        [doc.page_content for doc in retriever.invoke(code)[:top_k]]\n",
    "    )\n",
    "    examples_str = f\"\"\"\n",
    "You previously solved the following problems in this competition:\n",
    "<Examples>\n",
    "{examples_str}\n",
    "<Examples>\n",
    "Approach this new question with similar sophistication.\"\"\"\n",
    "    return {\"examples\": examples_str}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eef5c9b7-7dd3-4fbc-8f26-4220c9f7f2c8",
   "metadata": {},
   "source": [
    "**Builds the graph and adds an interrupt for human feedback**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 183,
   "id": "5763c50d-0a8c-400e-bc48-d9faf3ac5799",
   "metadata": {},
   "outputs": [],
   "source": [
    "builder = StateGraph(State)\n",
    "\n",
    "builder.add_node(\"draft\", draft_solver)\n",
    "builder.add_edge(START, \"draft\")\n",
    "builder.add_node(\"retrieve\", retrieve_examples)\n",
    "builder.add_node(\"solve\", solver)\n",
    "builder.add_node(\"modify\",modifier)\n",
    "builder.add_node(\"evaluate\", evaluate)\n",
    "builder.add_edge(\"draft\", \"retrieve\")\n",
    "builder.add_edge(\"retrieve\", \"solve\")\n",
    "#builder.add_edge(\"solve\", \"modify\")\n",
    "#builder.add_edge(\"modify\", \"evaluate\")\n",
    "builder.add_edge(\"solve\",\"evaluate\")\n",
    "\n",
    "\n",
    "def control_edge(state: State):\n",
    "    if state.get(\"status\") == \"success\":\n",
    "        return END\n",
    "    return \"solve\"\n",
    "\n",
    "\n",
    "builder.add_conditional_edges(\"evaluate\", control_edge, {END: END, \"solve\": \"solve\"})\n",
    "checkpointer = MemorySaver()\n",
    "\n",
    "graph = builder.compile(\n",
    "    checkpointer=checkpointer,\n",
    "    # New: this tells the graph to break any time it goes to the \"human\" node\n",
    "    interrupt_after=[\"evaluate\"],\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "827bd98e-6cee-48dd-8405-48ee1079a5d3",
   "metadata": {},
   "source": [
    "**Graph visualization**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 184,
   "id": "b4f3b81c-5811-49a9-b2b2-8d0a3fd3c891",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<IPython.core.display.Image object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from IPython.display import Image, display\n",
    "\n",
    "try:\n",
    "    display(Image(graph.get_graph().draw_mermaid_png()))\n",
    "except Exception:\n",
    "    # This requires some extra dependencies and is optional\n",
    "    pass"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7cf50a0e-4e3a-4be1-a3ab-f414bdbdbe88",
   "metadata": {},
   "source": [
    "**Removes huge test cases from input to save memory and tokens**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 185,
   "id": "bf8025a7-692a-4ad3-8da4-b935c1ed1ee8",
   "metadata": {},
   "outputs": [],
   "source": [
    "def _hide_test_cases(inputs):\n",
    "    copied = inputs.copy()\n",
    "    if \"test_cases\" in copied:\n",
    "        copied[\"test_cases\"] = [\n",
    "            {\n",
    "                \"inputs\": \"...\" if len(tc[\"inputs\"]) > 200 else tc[\"inputs\"],\n",
    "                \"outputs\": \"...\" if len(tc[\"outputs\"]) > 200 else tc[\"outputs\"]\n",
    "            }\n",
    "            for tc in copied[\"test_cases\"]\n",
    "        ]\n",
    "    else:\n",
    "        copied[\"test_cases\"] = \"...\"\n",
    "    return copied"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bd84870f-e743-462d-9935-c7794f3e307a",
   "metadata": {},
   "source": [
    "**Runs the code based on input**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 186,
   "id": "6616f693-cdc0-40ac-900f-31474da4a31d",
   "metadata": {},
   "outputs": [],
   "source": [
    "#indices of problems at each level\n",
    "levels = [[9, 10, 11, 21, 22, 23, 33, 34, 35, 45, 46, 47, 57, 58, 59, 69, 70, 71, 81, 82, 83, 93, 94, 95, 105, 106, 107, 117, 118, 119, 129, 130, 131, 141, 142, 143, 153, 154, 155, 165, 166, 167, 177, 178, 179, 189, 190, 191, 200, 201, 202, 212, 213, 214, 224, 225, 226, 236, 237, 238, 248, 249, 250, 260, 261, 262, 272, 273, 274, 284, 285, 286, 295, 296, 297, 307, 308, 309, 315, 316, 317, 318, 325, 326, 327, 334, 335, 336, 343, 344, 345, 352, 353, 354, 361, 362, 363, 370, 371, 372, 379, 380, 381, 382, 389, 390, 391, 398, 399, 400, 407, 408, 409, 416, 417, 418, 425, 426, 427, 434, 435, 436, 437, 444, 445, 446, 453, 454, 455, 462, 463, 464, 471, 472, 473, 480, 481, 482, 483, 493, 494, 495, 505, 506, 507, 517, 518, 519],\n",
    "            [6, 7, 8, 18, 19, 20, 30, 31, 32, 42, 43, 44, 54, 55, 56, 66, 67, 68, 78, 79, 80, 90, 91, 92, 102, 103, 104, 114, 115, 116, 126, 127, 128, 138, 139, 140, 150, 151, 152, 162, 163, 164, 174, 175, 176, 186, 187, 188, 197, 198, 199, 209, 210, 211, 221, 222, 223, 233, 234, 235, 245, 246, 247, 257, 258, 259, 269, 270, 271, 281, 282, 283, 292, 293, 294, 304, 305, 306, 312, 313, 314, 322, 323, 324, 331, 332, 333, 340, 341, 342, 349, 350, 351, 358, 359, 360, 367, 368, 369, 376, 377, 378, 386, 387, 388, 395, 396, 397, 404, 405, 406, 413, 414, 415, 422, 423, 424, 431, 432, 433, 441, 442, 443, 450, 451, 452, 459, 460, 461, 468, 469, 470, 477, 478, 479, 490, 491, 492, 502, 503, 504, 514, 515, 516], \n",
    "            [3, 4, 5, 15, 16, 17, 27, 28, 29, 39, 40, 41, 51, 52, 53, 63, 64, 65, 75, 76, 77, 87, 88, 89, 99, 100, 101, 111, 112, 113, 123, 124, 125, 135, 136, 137, 147, 148, 149, 159, 160, 161, 171, 172, 173, 183, 184, 185, 194, 195, 196, 206, 207, 208, 218, 219, 220, 230, 231, 232, 242, 243, 244, 254, 255, 256, 266, 267, 268, 278, 279, 280, 290, 291, 301, 302, 303, 310, 311, 319, 320, 321, 328, 329, 330, 337, 338, 339, 346, 347, 348, 355, 356, 357, 364, 365, 366, 373, 374, 375, 383, 384, 385, 392, 393, 394, 401, 402, 403, 410, 411, 412, 419, 420, 421, 428, 429, 430, 438, 439, 440, 447, 448, 449, 456, 457, 458, 465, 466, 467, 474, 475, 476, 487, 488, 489, 499, 500, 501, 511, 512, 513], \n",
    "            [0, 1, 2, 12, 13, 14, 24, 25, 26, 36, 37, 38, 48, 49, 50, 60, 61, 62, 72, 73, 74, 84, 85, 86, 96, 97, 98, 108, 109, 110, 120, 121, 122, 132, 133, 134, 144, 145, 146, 156, 157, 158, 168, 169, 170, 180, 181, 182, 192, 193, 203, 204, 205, 215, 216, 217, 227, 228, 229, 239, 240, 241, 251, 252, 253, 263, 264, 265, 275, 276, 277, 287, 288, 289, 298, 299, 300, 484, 485, 486, 496, 497, 498, 508, 509, 510]]\n",
    "#148 bronze\n",
    "#144 silver\n",
    "#142 gold\n",
    "#086 platinum\n",
    "#520 total\n",
    "\n",
    "def get_problem_ds(problem):\n",
    "    test_path = \"/Users/stevenyu/codeSheepcode/usaco_data/datasets/usaco_v3/tests/\" + problem[\"cp_id\"] + \"/\"\n",
    "    test_cases = []\n",
    "    for i in range(int(problem[\"num_tests\"])):\n",
    "        test_cases.append({\"inputs\":\"\", \"outputs\":\"\"})\n",
    "        inFN = test_path + \"I.\" + str(i+1)\n",
    "        outFN = test_path + \"O.\" + str(i+1)\n",
    "        with open(inFN, \"r\") as file:\n",
    "            test_cases[i][\"inputs\"] = file.read()\n",
    "        with open(outFN, \"r\") as file:\n",
    "            test_cases[i][\"outputs\"] = file.read()\n",
    "    return {\n",
    "        \"title\": problem[\"cp_id\"],\n",
    "        \"messages\": [(\"user\", problem[\"description\"])],\n",
    "        \"test_cases\": test_cases,\n",
    "        \"runtime_limit\": problem[\"runtime_limit\"],\n",
    "        \"status\": \"in_progress\",\n",
    "    }\n",
    "\n",
    "client = Client(hide_inputs=_hide_test_cases, hide_outputs=_hide_test_cases)\n",
    "\n",
    "def solve_no_interrupt(trials, problem):\n",
    "    '''Runs the solver 'trials' times without human feedback'''\n",
    "\n",
    "    config = {\"configurable\": {\"thread_id\": problem[\"title\"], \"k\": 2}}\n",
    "    \n",
    "\n",
    "    def concat_test_case(s: str):\n",
    "        print(s[:133])\n",
    "\n",
    "        pattern = r\"<test id=\\d+>(.*?)</test>\"\n",
    "        # Find all matches\n",
    "        matches = re.findall(pattern, s, re.DOTALL)\n",
    "\n",
    "        # Print a few characters from each test case\n",
    "        for i, match in enumerate(matches, start=1):\n",
    "            print(f\"Test Case {i}:\\n {match[:50]}...\") \n",
    "\n",
    "    def display_diagnostic():\n",
    "\n",
    "        most_recent_state = list(graph.get_state_history(config))[0]\n",
    "        snapshot = graph.get_state(most_recent_state.config)\n",
    "        ai_message = snapshot.values[\"messages\"][-2]\n",
    "        print(\"*\" * 35 + \" Previous Attempt Diagnostic \" + \"*\" * 35)\n",
    "        print(\"\\n\\nThought Process:\\n\\n\")\n",
    "        if ai_message.content:\n",
    "            print(ai_message.content)\n",
    "        print(\"\\n\\nCode:\\n\\n\")\n",
    "        if hasattr(ai_message, \"tool_calls\") and ai_message.tool_calls:\n",
    "            print(ai_message.tool_calls[0][\"args\"][\"code\"])\n",
    "        else:\n",
    "            print(filter_python_code(ai_message.content))\n",
    "        print(\"\\n\\nTest Case Results:\\n\\n\")\n",
    "        concat_test_case(snapshot.values[\"messages\"][-1].content)\n",
    "\n",
    "    #Runs the solver num_trials times\n",
    "    def run_solver(stream_state, num_trials, new_config):\n",
    "        with tracing_v2_enabled(client=client):\n",
    "            for _ in range(num_trials):\n",
    "                events = graph.stream(stream_state, new_config)\n",
    "                for event in events:\n",
    "                    for value in event.values():\n",
    "                        if type(value) == tuple:\n",
    "                            continue\n",
    "                        messages = value.get(\"messages\")\n",
    "                        if messages:\n",
    "                            if isinstance(messages, list):\n",
    "                                messages = value[\"messages\"][-1]\n",
    "                            print(\n",
    "                                \"Assistant:\",\n",
    "                                str(messages.content).replace(\"\\n\", \"\\\\n\")[:50],\n",
    "                            )\n",
    "                        elif value.get(\"examples\"):\n",
    "                            print(\"Retrieved examples:\\n\\n\", value[\"examples\"][:300] + \"...\")\n",
    "                        elif value.get(\"candidate\"):\n",
    "                            print(str(value[\"candidate\"].content)[:200])\n",
    "                most_recent_state = list(graph.get_state_history(config))[0]\n",
    "                snapshot = graph.get_state(most_recent_state.config)\n",
    "                print(snapshot.values[\"messages\"][-1].content[:133])\n",
    "                if graph.get_state(config).values[\"status\"] == \"success\":\n",
    "                    print(\"Code successful.\")\n",
    "                    display_diagnostic()\n",
    "                    return \"\"\n",
    "\n",
    "    run_solver(problem, 1, config) #first iteration\n",
    "    if graph.get_state(config).values[\"status\"] != \"success\":\n",
    "        run_solver(None, trials-1, config) #other trials-1 iterations\n",
    "        display_diagnostic()\n",
    "\n",
    "    return graph.get_state(config).values[\"status\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 187,
   "id": "9f1572d1-e0d3-4d82-92f5-0ef97ef2a72d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<generator object Pregel.stream at 0x316caa3e0>"
      ]
     },
     "execution_count": 187,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "graph.stream(None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 189,
   "id": "45910388-7187-4124-b938-d62e87169f24",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Farmer Nhoj dropped Bessie in the middle of nowhere! At time $t=0$, Bessie is\n",
      "located at $x=0$ on an infinite number line. She frantically searches for an\n",
      "exit by moving left or right by $1$ unit each second. However, there actually is\n",
      "no exit and after $T$ seconds, Bessie is back at $x=0$, tired and resigned. \n",
      "\n",
      "Farmer Nhoj tries to track Bessie but only knows how many times Bessie crosses\n",
      "$x=.5, 1.5, 2.5, \\ldots, (N-1).5$, given by an array $A_0,A_1,\\dots,A_{N-1}$\n",
      "($1\\leq N \\leq 10^5$, $1 \\leq A_i \\leq 10^6$, $\\sum A_i\\le 10^6$). Bessie never\n",
      "reaches $x>N$ nor\n",
      "$x<0$.\n",
      "\n",
      "In particular, Bessie's route can be represented by a string of\n",
      "$T = \\sum_{i=0}^{N-1} A_i$ $L$s and $R$s where the $i$th character represents\n",
      "the direction Bessie moves in during the $i$th second. The number of direction\n",
      "changes is defined as the number of occurrences of $LR$s plus the number of\n",
      "occurrences of $RL$s. \n",
      "\n",
      "Please help Farmer Nhoj find any route Bessie could have taken that is\n",
      "consistent with $A$ and minimizes the number of direction changes. It is\n",
      "guaranteed that there is at least one valid route.\n",
      "\n",
      "INPUT FORMAT (input arrives from the terminal / stdin):\n",
      "The first line contains $N$. The second line contains $A_0,A_1,\\dots,A_{N-1}$.\n",
      "\n",
      "OUTPUT FORMAT (print output to the terminal / stdout):\n",
      "Output a string $S$ of length $T = \\sum_{i=0}^{N-1} A_i$ where $S_i$ is $L$ or\n",
      "$R$, indicating the direction Bessie travels in during second $i$. If there are\n",
      "multiple routes minimizing the number of direction changes, output any.\n",
      "\n",
      "SAMPLE INPUT:\n",
      "2\n",
      "2 4\n",
      "SAMPLE OUTPUT: \n",
      "RRLRLL\n",
      "\n",
      "There is only 1 valid route, corresponding to the route\n",
      "$0\\to 1 \\to 2 \\to 1\\to 2 \\to 1\\to 0$. Since this is the only possible route, it\n",
      "also has the minimum number of direction changes.\n",
      "\n",
      "SAMPLE INPUT:\n",
      "3\n",
      "2 4 4\n",
      "SAMPLE OUTPUT: \n",
      "RRRLLRRLLL\n",
      "\n",
      "There are 3 possible routes:\n",
      "\n",
      "\n",
      "RRLRRLRLLL\n",
      "RRRLRLLRLL\n",
      "RRRLLRRLLL\n",
      "\n",
      "The first two routes have 5 direction changes, while the last one has only 3. \n",
      "Thus the last route is the only correct output.\n",
      "\n",
      "SCORING:\n",
      "Inputs 3-5: $N\\le 2$Inputs 3-10: $T = A_0 + A_1 + \\cdots + A_{N-1} \\leq 5000$Inputs 11-20: No additional constraints.\n",
      "\n",
      "\n",
      "Problem credits: Brandon Wang and Claire Zhang\n",
      "\n",
      "silver\n",
      "[{'id': 'toolu_0187iSUtkqze7u1dCbdyrw4v', 'input': {'reasoning': 'To find a route that minimizes the number of direction changes while being consistent with the given crossing counts, we can use a gre\n",
      "Retrieved examples:\n",
      "\n",
      " \n",
      "You previously solved the following problems in this competition:\n",
      "<Examples>\n",
      "<problem>\n",
      "\n",
      "**Note: The time limit for this problem is 4s, 2x the default.**\n",
      "Farmer John's $N$ cows ($1\\le N\\le 1.5\\cdot 10^5$) have integer milk production\n",
      "values $a_1,\\dots,a_N$.  That is, the $i$th cow produces $a_i$ uni...\n",
      "Assistant: \n",
      "Assistant: Incorrect submission. Please respond with updated \n",
      "Incorrect submission. Please respond with updated code.\n",
      "Pass rate: 1/20\n",
      "Results:\n",
      "<test id=0>\n",
      "wrong answer. Expected 'RRLRLL\n",
      "', got 'R\n",
      "Assistant: \n",
      "Assistant: Incorrect submission. Please respond with updated \n",
      "Incorrect submission. Please respond with updated code.\n",
      "Pass rate: 1/20\n",
      "Results:\n",
      "<test id=0>\n",
      "wrong answer. Expected 'RRLRLL\n",
      "', got 'R\n",
      "Assistant: \n",
      "Assistant: Incorrect submission. Please respond with updated \n",
      "Incorrect submission. Please respond with updated code.\n",
      "Pass rate: 1/20\n",
      "Results:\n",
      "<test id=0>\n",
      "wrong answer. Expected 'RRLRLL\n",
      "', got 'R\n",
      "*********************************** Previous Attempt Diagnostic ***********************************\n",
      "\n",
      "\n",
      "Thought Process:\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Code:\n",
      "\n",
      "\n",
      "def find_minimal_route(N, A):\n",
      "    # Initialize variables\n",
      "    direction = 'R'\n",
      "    result = []\n",
      "\n",
      "    for i in range(N):\n",
      "        if direction == 'R':\n",
      "            result.extend(['R'] * A[i])\n",
      "            direction = 'L'\n",
      "        else:\n",
      "            result.extend(['L'] * A[i])\n",
      "            direction = 'R'\n",
      "\n",
      "    return ''.join(result)\n",
      "\n",
      "# Read input\n",
      "def main():\n",
      "    import sys\n",
      "    input = sys.stdin.read\n",
      "    data = input().split()\n",
      "    \n",
      "    N = int(data[0])\n",
      "    A = list(map(int, data[1:]))\n",
      "\n",
      "    # Get the minimal route\n",
      "    route = find_minimal_route(N, A)\n",
      "\n",
      "    # Print the result\n",
      "    print(route)\n",
      "\n",
      "if __name__ == \"__main__\":\n",
      "    main()\n",
      "\n",
      "\n",
      "Test Case Results:\n",
      "\n",
      "\n",
      "Incorrect submission. Please respond with updated code.\n",
      "Pass rate: 1/20\n",
      "Results:\n",
      "<test id=0>\n",
      "wrong answer. Expected 'RRLRLL\n",
      "', got 'R\n",
      "Test Case 1:\n",
      " \n",
      "wrong answer. Expected 'RRLRLL\n",
      "', got 'RRLLLL\n",
      "'\n",
      "...\n",
      "Test Case 2:\n",
      " \n",
      "wrong answer. Expected 'RRRLLRRLLL\n",
      "', got 'RRLLLL...\n",
      "Test Case 3:\n",
      " \n",
      "wrong answer. Expected 'RLRLRLRLRLRLRLRLRLRLRLRLR...\n",
      "Test Case 4:\n",
      " \n",
      "wrong answer. Expected 'RRLRLRLRLRLRLRLRLRLRLRLRL...\n",
      "Test Case 5:\n",
      " \n",
      "wrong answer. Expected 'RRLLRRLLRRLLRRLLRRLLRRLLR...\n",
      "Test Case 6:\n",
      " \n",
      "wrong answer. Expected 'RRRRRRRRRRRRRRRRRRRRRRRRR...\n",
      "Test Case 7:\n",
      " \n",
      "wrong answer. Expected 'RRRRRRRRRRRRRRRRRRRRRRRRR...\n",
      "Test Case 8:\n",
      " \n",
      "wrong answer. Expected 'RRRRRRRRRRRRRRRRRRRRRRRRR...\n",
      "Test Case 9:\n",
      " \n",
      "wrong answer. Expected 'RRRRRRRRRRRRRRRRRRRRRRRRR...\n",
      "Test Case 10:\n",
      " \n",
      "wrong answer. Expected 'RRRRRRRRRRRRRRRRRRRRRRRRR...\n",
      "Test Case 11:\n",
      " \n",
      "passed\n",
      "...\n",
      "Test Case 12:\n",
      " \n",
      "timed out\n",
      "...\n",
      "Test Case 13:\n",
      " \n",
      "timed out\n",
      "...\n",
      "Test Case 14:\n",
      " \n",
      "timed out\n",
      "...\n",
      "Test Case 15:\n",
      " \n",
      "timed out\n",
      "...\n",
      "Test Case 16:\n",
      " \n",
      "timed out\n",
      "...\n",
      "Test Case 17:\n",
      " \n",
      "timed out\n",
      "...\n",
      "Test Case 18:\n",
      " \n",
      "timed out\n",
      "...\n",
      "Test Case 19:\n",
      " \n",
      "timed out\n",
      "...\n",
      "Test Case 20:\n",
      " \n",
      "timed out\n",
      "...\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'in_progress'"
      ]
     },
     "execution_count": 189,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Sample of 4 problems of each difficulty\n",
    "sample = [[9,102,125],\n",
    "          [8,9,82],\n",
    "          [35,79,131],\n",
    "          [6,29,62]]\n",
    " \n",
    "row = ds[levels[1][8]]\n",
    "print(row[\"description\"])\n",
    "print(row[\"problem_level\"])\n",
    "\n",
    "row_input = get_problem_ds(row)\n",
    "solve_no_interrupt(3, row_input)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b0d2ef56-1a2c-498b-b648-acd4b1b71368",
   "metadata": {},
   "source": [
    "**User feedback**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "37c6203e-1345-41ac-9a91-b588d57c7860",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'config' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[52], line 2\u001b[0m\n\u001b[1;32m      1\u001b[0m updated_config \u001b[38;5;241m=\u001b[39m graph\u001b[38;5;241m.\u001b[39mupdate_state(\n\u001b[0;32m----> 2\u001b[0m     \u001b[43mconfig\u001b[49m,\n\u001b[1;32m      3\u001b[0m     values\u001b[38;5;241m=\u001b[39m{\n\u001b[1;32m      4\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmessages\u001b[39m\u001b[38;5;124m\"\u001b[39m: [\n\u001b[1;32m      5\u001b[0m             (\n\u001b[1;32m      6\u001b[0m                 \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124muser\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[1;32m      7\u001b[0m \u001b[38;5;250m                \u001b[39m\u001b[38;5;124;03m\"\"\"User Message Goes Here\"\"\"\u001b[39;00m,\n\u001b[1;32m      8\u001b[0m             )\n\u001b[1;32m      9\u001b[0m         ]\n\u001b[1;32m     10\u001b[0m     },\n\u001b[1;32m     11\u001b[0m )\n",
      "\u001b[0;31mNameError\u001b[0m: name 'config' is not defined"
     ]
    }
   ],
   "source": [
    "updated_config = graph.update_state(\n",
    "    config,\n",
    "    values={\n",
    "        \"messages\": [\n",
    "            (\n",
    "                \"user\",\n",
    "                \"\"\"User Message Goes Here\"\"\",\n",
    "            )\n",
    "        ]\n",
    "    },\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3af87668-045d-4c02-bdcd-82ef932de4e8",
   "metadata": {},
   "source": [
    "**Does a certain number of solve iterations based on the user's suggestions.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4ce71761-8a76-43f4-b977-b0a07719523b",
   "metadata": {},
   "outputs": [],
   "source": [
    "num_trials = 1\n",
    "with tracing_v2_enabled(client=client):\n",
    "    for _ in range(num_trials):\n",
    "        events = graph.stream(None, updated_config)\n",
    "        for event in events:\n",
    "            for value in event.values():\n",
    "                messages = value.[\"messages\"]\n",
    "                if messages:\n",
    "                    if isinstance(messages, list):\n",
    "                        messages = value[\"messages\"][-1]\n",
    "                    print(\n",
    "                        \"Assistant:\",\n",
    "                        str(messages.content).replace(\"\\n\", \"\\\\n\")[:50],\n",
    "                    )\n",
    "                elif value.get(\"examples\"):\n",
    "                    print(\"Retrieved examples:\\n\\n\", value[\"examples\"][:100] + \"...\")\n",
    "                elif value.get(\"candidate\"):\n",
    "                    print(str(value[\"candidate\"].content)[:200])\n",
    "        if graph.get_state(config).values[\"status\"] == \"success\":\n",
    "            break\n",
    "        print(\"Continuing...\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
